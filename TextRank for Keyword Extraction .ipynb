{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T11:30:56.647913Z",
     "start_time": "2021-01-25T11:30:54.562844Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T11:30:56.657865Z",
     "start_time": "2021-01-25T11:30:56.652514Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = \"\"\" \n",
    "Washington (CNN)President Joe Biden on Monday will reinstate the Covid-19 travel restrictions on non-US citizens who have been in Brazil, Ireland, the United Kingdom, and much of Europe, a White House official confirmed to CNN.\n",
    "Biden will also extend the restrictions to travelers who have recently been to South Africa, the official said.\n",
    "The step, which was first reported by Reuters, comes just one week after President Donald Trump signed an executive order in his final days in office lifting the restrictions on travelers from these countries effective January 26.\n",
    "\n",
    "\"I agree with the Secretary that this action is the best way to continue protecting Americans from COVID-19 while enabling travel to resume safely,\" Trump wrote in the order, referring to then-Secretary of Health and Human Services Alex Azar.\n",
    "The Biden transition team, however, vowed that same night the new administration would not lift the restrictions. \"With the pandemic worsening, and more contagious variants emerging around the world, this is not the time to be lifting restrictions on international travel,\" then-incoming White House press secretary Jen Psaki said on Twitter.\n",
    "\"On the advice of our medical team, the Administration does not intend to lift these restrictions on 1/26. In fact, we plan to strengthen public health measures around international travel in order to further mitigate the spread of COVID-19.\"\n",
    "The decision to reinstate the travel restrictions -- and expand restrictions in the case of South Africa -- marks the latest effort by the Biden administration to break from Trump's discursive approach to the pandemic as cases continue to climb nationwide.\n",
    "Biden said on his first full day in office on Thursday his strategy would be \"based on science, not politics\" as he signed a slate of coronavirus-related executive actions, including ramping up vaccination supplies and requiring international travelers to provide proof of a negative Covid-19 test prior to traveling to the US.\n",
    "Many of the countries that would have been impacted by Trump's order have their own recent requirements for American travelers looking to enter their borders.\n",
    "\n",
    "US travelers must have a negative Covid-19 test from within 72 hours prior to travel into the United Kingdom or Ireland, and in conjunction with proof of a completed Declaration of Traveler's Health to enter Brazil. American travelers generally cannot enter countries such as Spain, Germany, France, Italy and Sweden without meeting specific requirements.\n",
    "This story has been updated to include additional information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T11:30:56.838147Z",
     "start_time": "2021-01-25T11:30:56.664858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restrictions - 3.3633258501419268\n",
      "travel - 3.0576459949639263\n",
      "Trump - 2.5227211228697217\n",
      "travelers - 2.473966595774661\n",
      "Biden - 2.446352814634349\n",
      "order - 2.4250339713256577\n",
      "Covid-19 - 1.4423685359206253\n",
      "White - 1.440195629660673\n",
      "office - 1.4044921654831661\n",
      "Health - 1.399671356593617\n",
      "House - 1.3890055755481185\n",
      "Ireland - 1.3379469472494345\n"
     ]
    }
   ],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(doc, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T11:31:08.972128Z",
     "start_time": "2021-01-25T11:30:56.845829Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from graph_show import GraphShow\n",
    "from textrank import TextRank\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "class NewsMining():\n",
    "    \"\"\"News Mining\"\"\"\n",
    "    def __init__(self):\n",
    "        self.textranker = TextRank()\n",
    "        self.ners = ['PERSON', 'ORG', 'GPE']\n",
    "        self.ner_dict = {\n",
    "            'PERSON': 'Person',  # People, including fictional\n",
    "            'ORG': 'Organization',  # Companies, agencies, institutions, etc.\n",
    "            'GPE': 'Location',  # Countries, cities, states.\n",
    "        }\n",
    "        # dependency markers for subjects\n",
    "        self.SUBJECTS = {\"nsubj\", \"nsubjpass\",\n",
    "                         \"csubj\", \"csubjpass\", \"agent\", \"expl\"}\n",
    "        # dependency markers for objects\n",
    "        self.OBJECTS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
    "\n",
    "        self.graph_shower = GraphShow()\n",
    "\n",
    "    def clean_spaces(self, s):\n",
    "        s = s.replace('\\r', '')\n",
    "        s = s.replace('\\t', ' ')\n",
    "        s = s.replace('\\n', ' ')\n",
    "        return s\n",
    "\n",
    "    def remove_noisy(self, content):\n",
    "        \"\"\"Remove brackets\"\"\"\n",
    "        p1 = re.compile(r'（[^）]*）')\n",
    "        p2 = re.compile(r'\\([^\\)]*\\)')\n",
    "        return p2.sub('', p1.sub('', content))\n",
    "\n",
    "    def collect_ners(self, ents):\n",
    "        \"\"\"Collect token only with PERSON, ORG, GPE\"\"\"\n",
    "        collected_ners = []\n",
    "        for token in ents:\n",
    "            if token.label_ in self.ners:\n",
    "                collected_ners.append(token.text + '/' + token.label_)\n",
    "        return collected_ners\n",
    "\n",
    "    def conll_syntax(self, sent):\n",
    "        \"\"\"Convert one sentence to conll format.\"\"\"\n",
    "\n",
    "        tuples = list()\n",
    "        for word in sent:\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                head_idx = word.head.i + 1\n",
    "            tuples.append([word.i + 1,  # Current word index, begin with 1\n",
    "                           word.text,  # Word\n",
    "                           word.lemma_,  # Lemma\n",
    "                           word.pos_,  # Coarse-grained tag\n",
    "                           word.tag_,  # Fine-grained tag\n",
    "                           '_',\n",
    "                           head_idx,  # Head of current  Index\n",
    "                           word.dep_,  # Relation\n",
    "                           '_', '_'])\n",
    "        return tuples\n",
    "\n",
    "    def syntax_parse(self, sent):\n",
    "        \"\"\"Convert one sentence to conll format.\"\"\"\n",
    "        tuples = list()\n",
    "        for word in sent:\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                head_idx = word.head.i + 1\n",
    "            tuples.append([word.i + 1,  # Current word index, begin with 1\n",
    "                           word.text,  # Word\n",
    "                           word.pos_,  # Coarse-grained tag\n",
    "                           word.head,\n",
    "                           head_idx,  # Head of current  Index\n",
    "                           word.dep_,  # Relation\n",
    "                           ])\n",
    "        return tuples\n",
    "\n",
    "    def build_parse_chile_dict(self, sent, tuples):\n",
    "        child_dict_list = list()\n",
    "        for word in sent:\n",
    "            child_dict = dict()\n",
    "            for arc in tuples:\n",
    "                if arc[3] == word:\n",
    "                    if arc[-1] in child_dict:\n",
    "                        child_dict[arc[-1]].append(arc)\n",
    "                    else:\n",
    "                        child_dict[arc[-1]] = []\n",
    "                        child_dict[arc[-1]].append(arc)\n",
    "            child_dict_list.append([word, word.pos_, word.i, child_dict])\n",
    "        return child_dict_list\n",
    "\n",
    "    def complete_VOB(self, verb, child_dict_list):\n",
    "        '''Find VOB by SBV'''\n",
    "        for child in child_dict_list:\n",
    "            word = child[0]\n",
    "            # child_dict: {'dobj': [[7, 'startup', 'NOUN', buying, 5, 'dobj']], 'prep': [[8, 'for', 'ADP', buying, 5, 'prep']]}\n",
    "            child_dict = child[3]\n",
    "            if word == verb:\n",
    "                for object_type in self.OBJECTS:  # object_type: 'dobj'\n",
    "                    if object_type not in child_dict:\n",
    "                        continue\n",
    "                    # [7, 'startup', 'NOUN', buying, 5, 'dobj']\n",
    "                    vob = child_dict[object_type][0]\n",
    "                    obj = vob[1]  # 'startup'\n",
    "                    return obj\n",
    "        return ''\n",
    "\n",
    "    def extract_triples(self, sent):\n",
    "        svo = []\n",
    "        tuples = self.syntax_parse(sent)\n",
    "        child_dict_list = self.build_parse_chile_dict(sent, tuples)\n",
    "        for tuple in tuples:\n",
    "            rel = tuple[-1]\n",
    "            if rel in self.SUBJECTS:\n",
    "                sub_wd = tuple[1]\n",
    "                verb_wd = tuple[3]\n",
    "                obj = self.complete_VOB(verb_wd, child_dict_list)\n",
    "                subj = sub_wd\n",
    "                verb = verb_wd.text\n",
    "                if not obj:\n",
    "                    svo.append([subj, verb])\n",
    "                else:\n",
    "                    svo.append([subj, verb+' '+obj])\n",
    "        return svo\n",
    "\n",
    "    def extract_keywords(self, words_postags):\n",
    "        return self.textranker.extract_keywords(words_postags, 10)\n",
    "\n",
    "    def collect_coexist(self, ner_sents, ners):\n",
    "        \"\"\"Construct NER co-occurrence matrices\"\"\"\n",
    "        co_list = []\n",
    "        for words in ner_sents:\n",
    "            co_ners = set(ners).intersection(set(words))\n",
    "            co_info = self.combination(list(co_ners))\n",
    "            co_list += co_info\n",
    "        if not co_list:\n",
    "            return []\n",
    "        return {i[0]: i[1] for i in Counter(co_list).most_common()}\n",
    "\n",
    "    def combination(self, a):\n",
    "        '''list all combination'''\n",
    "        combines = []\n",
    "        if len(a) == 0:\n",
    "            return []\n",
    "        for i in a:\n",
    "            for j in a:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                combines.append('@'.join([i, j]))\n",
    "        return combines\n",
    "\n",
    "    def main(self, content):\n",
    "        '''Main function'''\n",
    "        if not content:\n",
    "            return []\n",
    "\n",
    "        words_postags = []  # token and its POS tag\n",
    "        ner_sents = []      # store sentences which contain NER entity\n",
    "        ners = []           # store all NER entity from whole article\n",
    "        triples = []        # store subject verb object\n",
    "        events = []         # store events\n",
    "\n",
    "        # 01 remove linebreaks and brackets\n",
    "        content = self.remove_noisy(content)\n",
    "        content = self.clean_spaces(content)\n",
    "\n",
    "        # 02 split to sentences\n",
    "        doc = nlp(content)\n",
    "\n",
    "        for i, sent in enumerate(doc.sents):\n",
    "            words_postags = [[token.text, token.pos_] for token in sent]\n",
    "            words = [token.text for token in sent]\n",
    "            postags = [token.pos_ for token in sent]\n",
    "            ents = nlp(sent.text).ents  # NER detection\n",
    "            collected_ners = self.collect_ners(ents)\n",
    "\n",
    "            if collected_ners:  # only extract triples when the sentence contains 'PERSON', 'ORG', 'GPE'\n",
    "                triple = self.extract_triples(sent)\n",
    "                if not triple:\n",
    "                    continue\n",
    "                triples += triple\n",
    "                ners += collected_ners\n",
    "                ner_sents.append(\n",
    "                    [token.text + '/' + token.label_ for token in sent.ents])\n",
    "\n",
    "        # 03 get keywords\n",
    "        keywords = [i[0] for i in self.extract_keywords(words_postags)]\n",
    "        for keyword in keywords:\n",
    "            name = keyword\n",
    "            cate = 'keyword'\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 04 add triples to event only the word in keyword\n",
    "        for t in triples:\n",
    "            if (t[0] in keywords or t[1] in keywords) and len(t[0]) > 1 and len(t[1]) > 1:\n",
    "                events.append([t[0], t[1]])\n",
    "\n",
    "        # 05 get word frequency and add to events\n",
    "        word_dict = [i for i in Counter([i[0] for i in words_postags if i[1] in [\n",
    "                                        'NOUN', 'PROPN', 'VERB'] and len(i[0]) > 1]).most_common()][:10]\n",
    "        for wd in word_dict:\n",
    "            name = wd[0]\n",
    "            cate = 'frequency'\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 06 get NER from whole article\n",
    "        ner_dict = {i[0]: i[1] for i in Counter(ners).most_common(20)}\n",
    "        for ner in ner_dict:\n",
    "            name = ner.split('/')[0]  # Jessica Miller\n",
    "            cate = self.ner_dict[ner.split('/')[1]]  # PERSON\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 07 get all NER entity co-occurrence information\n",
    "        # here ner_dict is from above 06\n",
    "        co_dict = self.collect_coexist(ner_sents, list(ner_dict.keys()))\n",
    "        co_events = [[i.split('@')[0].split(\n",
    "            '/')[0], i.split('@')[1].split('/')[0]] for i in co_dict]\n",
    "        events += co_events\n",
    "\n",
    "        # 08 show event graph\n",
    "        self.graph_shower.create_page(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T11:31:09.402765Z",
     "start_time": "2021-01-25T11:31:08.974793Z"
    }
   },
   "outputs": [],
   "source": [
    "Miner = NewsMining()\n",
    "Miner.main(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
